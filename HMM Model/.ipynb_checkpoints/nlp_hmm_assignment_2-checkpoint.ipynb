{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural Language Processing Assignment\n",
    "Author: Raeed Asif\n",
    "PoS Tagging with Hidden Markov Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importing nltk and brown corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "download if! NTLK brown and google universal_tagset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/raeedasif/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /Users/raeedasif/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "retrieving tagged_sentences from brown of category 'news' and with tagset of google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_sentences = brown.tagged_sents(categories='news', tagset = 'universal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tagged_sentences[:-500] # training set\n",
    "test_data = tagged_sentences[-500:] # test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extracting words and tags as a seperate list from test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "untagged_words = [word for sent in tagged_sentences for word, tag in sent]\n",
    "test_base_tags = [tag for sent in tagged_sentences for word, tag in sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Markov Model Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training HMM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import hmm\n",
    "trainer = hmm.HiddenMarkovModelTrainer()\n",
    "tagger = trainer.train_supervised(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing HMM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=[]\n",
    "temp2=[]\n",
    "hmm_tagged_seq=[]\n",
    "for word in untagged_words:\n",
    "    temp_seq = tagger.tag(word.split())\n",
    "    temp.append(temp_seq)\n",
    "    \n",
    "temp1 = [j for sub in temp for j in sub]\n",
    "for j in temp1:\n",
    "    temp2.append([i.replace('(', '') for i in j])\n",
    "temp3 = [j for sub in temp2 for j in sub]\n",
    "for i in range(1,len(temp3),2):\n",
    "    hmm_tagged_seq.append(temp3[i])\n",
    "#print(hmm_tagged_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_test_check_hmm = [i for i, j in zip(test_base_tags, hmm_tagged_seq) if i == j]\n",
    "acc = ((len(sample_test_check_hmm))/len(hmm_tagged_seq)*100)\n",
    "#print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vertibi algorithim "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adding a START and END tag in the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_words = [ ]\n",
    "all_tags = [ ]\n",
    "\n",
    "for sent in train_data:\n",
    "    tagged_words.append( (\"START\", \"START\") )\n",
    "    all_tags.append(\"START\")\n",
    "    for (word, tag) in sent:\n",
    "        all_tags.append(tag)\n",
    "        tagged_words.append( (tag, word) )\n",
    "    tagged_words.append( (\"END\", \"END\") )\n",
    "    all_tags.append(\"END\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## trasition probabilities [ P(t_{i} | t_{i-1}) = C(t_{i-1}, t_{i})/C(t_{i-1}) ]: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd_tags= nltk.ConditionalFreqDist(nltk.bigrams(all_tags)) # C(t_{i-1}, t_{i}):\n",
    "cpd_tags = nltk.ConditionalProbDist(cfd_tags, nltk.MLEProbDist) # P(t_{i} | t_{i-1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## emission probabilities [ P(w_{i} | t_{i}) =  C(t_{i}, w_{i}) / C(t_{i}) ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd_tagwords = nltk.ConditionalFreqDist(tagged_words) # C(t_{i}, w_{i})\n",
    "cpd_tagwords = nltk.ConditionalProbDist(cfd_tagwords, nltk.MLEProbDist)  # P(w_{i} | t_{i})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vertibi Algorithm:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def viterbi(sentence):\n",
    "    # Step 1. initialization step\n",
    "    distinct_tags = np.array(list(set(all_tags)))\n",
    "    tagslen = len(distinct_tags)\n",
    "\n",
    "    sentlen = len(sentence)\n",
    "    viterbi = np.zeros((tagslen, sentlen+1) ,dtype=float)\n",
    "    backpointer = np.zeros((tagslen, sentlen+1) ,dtype=np.uint32)\n",
    "\n",
    "\n",
    "\n",
    "    # Step 1. initialization step\n",
    "    for s, tag in enumerate(distinct_tags):\n",
    "        viterbi[s,0] =  cpd_tags[\"START\"].prob(tag) * cpd_tagwords[tag].prob( sentence[0] )\n",
    "        backpointer[s,0] = 0\n",
    "        #print(\"Viterbi probability V( {1} ,{0} ) = {2}\".format(sentence[0],tag,  viterbi[s,0]) )\n",
    "    #print('============================')\n",
    "\n",
    "    # Step 2. recursion step\n",
    "    for t in range(1, sentlen):\n",
    "        for s, tag in enumerate(distinct_tags):\n",
    "            current_viterbi = np.zeros( tagslen ,dtype=float)\n",
    "            for sprime, predtag in enumerate(distinct_tags):\n",
    "                current_viterbi[sprime] = viterbi[sprime,t-1] * \\\n",
    "                                          cpd_tags[predtag].prob(tag) * \\\n",
    "                                          cpd_tagwords[tag].prob(sentence[t])\n",
    "            backpointer[s,t] = np.argmax(current_viterbi)\n",
    "            viterbi[s,t] = max(current_viterbi)\n",
    "            #print(\"Viterbi probability V( {1} ,{0} ) = {2}\".format(sentence[t],tag,  viterbi[s,t]))\n",
    "\n",
    "        #print('============================')\n",
    "\n",
    "\n",
    "    # Step 3. termination step\n",
    "    current_viterbi = np.empty( tagslen ,dtype=float)\n",
    "    ind_of_end = -1\n",
    "    for s, tag in enumerate(distinct_tags):\n",
    "        if tag == \"END\":\n",
    "            ind_of_end  = s\n",
    "        current_viterbi[s] = viterbi[s,sentlen-1] * cpd_tags[tag].prob(\"END\")\n",
    "\n",
    "    backpointer[ind_of_end,sentlen] = np.argmax(current_viterbi)\n",
    "    viterbi[ind_of_end,sentlen] = max(current_viterbi)\n",
    "\n",
    "    # Step 3. backtrace the path\n",
    "    best_tagsequence = [ ]\n",
    "    prob_tagsequence = viterbi[ind_of_end,sentlen]\n",
    "    prevind  = ind_of_end\n",
    "    for t in range(sentlen,0,-1):\n",
    "        prevind = backpointer[prevind,t]\n",
    "        best_tagsequence.append(distinct_tags[prevind])\n",
    "    best_tagsequence.reverse()\n",
    "\n",
    "    return best_tagsequence, prob_tagsequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing set on vertibi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/raeedasif/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "v_tagged_seq=[]\n",
    "\n",
    "for word in untagged_words:\n",
    "\tsentence =  nltk.word_tokenize(word)\n",
    "\tbest_tagsequence,prob_tagsequence = viterbi(sentence)\n",
    "\tv_tagged_seq.append(best_tagsequence)\n",
    "\n",
    "v_tagged_seq_flatten = [j for sub in v_tagged_seq for j in sub]\n",
    "\n",
    "#print(v_tagged_seq_flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_test_check = [i for i, j in zip(v_tagged_seq_flatten, test_base_tags) if i == j]\n",
    "acc = ((len(sample_test_check))/len(v_tagged_seq_flatten)*100)\n",
    "#print(\"accuracy:\",(acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
