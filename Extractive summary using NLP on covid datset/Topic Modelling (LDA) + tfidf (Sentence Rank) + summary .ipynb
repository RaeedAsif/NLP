{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Notebook for TOPIC Modelling (LDA), calculating TF-IDFs, Sentence ranking and extractive summary\n",
    "\n",
    "## Name: Raeed Asif\n",
    "## Class: BSCS 6B\n",
    "## Reg# 199323"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run these to import necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import spacy\n",
    "import math\n",
    "import statistics \n",
    "\n",
    "from statistics import mean, median, mode, stdev\n",
    "\n",
    "\n",
    "# word cloud\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "# Gensim tools\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# nltk tools\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "# matplot lib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biorxiv_clean = pd.read_csv(\"data/biorxiv_clean.csv\")\n",
    "biorxiv_clean.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cord-19-solution-toolbox: https://www.kaggle.com/gpreda/cord-19-solution-toolbox\n",
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "def show_wordcloud(data, title = None):\n",
    "    wordcloud = WordCloud(background_color='white',\n",
    "                          stopwords=stopwords,\n",
    "                          max_words=1000,\n",
    "                          max_font_size=40,\n",
    "                          scale=5,\n",
    "                          random_state=1).generate(str(data))\n",
    "    \n",
    "    fig = plt.figure( 1 ,figsize=(15,15) )\n",
    "    plt.axis('off')\n",
    "    \n",
    "    if title: \n",
    "        fig.suptitle(title, fontsize=14)\n",
    "        fig.subplots_adjust(top=2.3)\n",
    "        \n",
    "    plt.imshow(wordcloud)\n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of world model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_wordcloud(biorxiv_clean[\"abstract\"], title=\"Wordcloud of abstracts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_wordcloud(biorxiv_clean[\"text\"], title=\"Wordcloud of text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentences = [text.lower().split() for text in biorxiv_clean[\"text\"]]\n",
    "#print(sentences[0])\n",
    "\n",
    "#Text to list\n",
    "df = biorxiv_clean\n",
    "df = df.text.dropna()\n",
    "data = df.tolist()\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  \n",
    "\n",
    "\n",
    "data_words = list(sent_to_words(data))     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biagram objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating biagram model to make biagram pairs of data_words\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=20)\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing, Bigrams, lematization and required parameter for LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['preprint','copyright','medrxiv','https_doi','get','copyright_holder','peer','reviewed','https','org','rights_reserved', ''])\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    texts_out = []\n",
    "    nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        #print(doc)\n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "def get_corpus(data_list):\n",
    "    words = data_list\n",
    "    data_words_nostops = remove_stopwords(words)\n",
    "    data_words_bigrams = bigrams(data_words_nostops)\n",
    "    data_words_lematized = lemmatization(data_words_bigrams)\n",
    "    id2word = gensim.corpora.Dictionary(data_words_lematized)\n",
    "    id2word.filter_extremes(no_below=10, no_above=0.35)\n",
    "    id2word.compactify()\n",
    "    corpus = [id2word.doc2bow(text) for text in data_words_lematized]\n",
    "    return corpus, id2word, bigram, data_words_lematized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus, id2word, bigram, data_lematized = get_corpus(data_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 5\n",
    "random_state=100\n",
    "\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=num_topics, \n",
    "                                           random_state=random_state,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lematized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Distance Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(vis, './lda4topics_v2.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " print(lda_model.show_topic(2)) #5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filteing papers and extarcting text, [\"coronavirus\",\"tissue\",\"immune\",\"disease\",\"gene\",\"drug\"] are those under same topic as seen in LDA model above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter papers containing all words in list\n",
    "def filter_papers_word_list(word_list):\n",
    "    papers_id_list = []\n",
    "    for idx, paper in biorxiv_clean.iterrows():\n",
    "        if all(x in paper.text for x in word_list):\n",
    "            papers_id_list.append(paper.paper_id)\n",
    "\n",
    "    return papers_id_list\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 100000) # Extend the display width to prevent split functions to not cover full text\n",
    "biorxiv_environment = filter_papers_word_list([\"coronavirus\",\"tissue\",\"immune\",\"disease\",\"gene\",\"drug\"])\n",
    "print(\"Papers containing coronavirus: \", len(biorxiv_environment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_conclusion(df, papers_id_list):\n",
    "    data = df.loc[df['paper_id'].isin(papers_id_list)]\n",
    "    conclusion = []\n",
    "    for idx, paper in data.iterrows():\n",
    "        paper_text = paper.text\n",
    "        if \"\\nConclusion\\n\" in paper.text:\n",
    "            conclusion.append(paper_text.split('\\nConclusion\\n')[1])\n",
    "        else:\n",
    "            conclusion.append(\"No Conclusion section\")\n",
    "    data['conclusion'] = conclusion\n",
    "        \n",
    "    return data\n",
    "\n",
    "pd.reset_option('^display.', silent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing before computing TF-IDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(text):\n",
    "    return text.replace('\\n','.')\n",
    "\n",
    "def remove_stopwords_2(texts):\n",
    "    return [word for word in simple_preprocess(str(texts)) if word not in stop_words]\n",
    "\n",
    "environ_trans_conclusion = extract_conclusion(biorxiv_clean, biorxiv_environment)\n",
    "environ_trans_conclusion[\"text\"]\n",
    "list_old = [split(x) for x in environ_trans_conclusion[\"text\"]]\n",
    "\n",
    "list_new_1 = [x.split(\".\") for x in list_old]\n",
    "\n",
    "lematize_list =[]\n",
    "for idx,doc in enumerate(list_new_1):\n",
    "    lematize_temp_list = []\n",
    "    stopword_list = []\n",
    "    for line in doc:\n",
    "        stopword_list.append(remove_stopwords_2(line))\n",
    "    lematize_list.append(lemmatization(stopword_list))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_new = [x for x in environ_trans_conclusion[\"text\"]]\n",
    "list_new_nostops = remove_stopwords(list_new)\n",
    "list_new_lematized = lemmatization(list_new_nostops)\n",
    "#print(list_new_lematized)\n",
    "lis = []\n",
    "numOfWords={}\n",
    "for idx,lis in enumerate(list_new_lematized):\n",
    "    numOfWords[idx] = dict.fromkeys(lis, 0)\n",
    "    tmp ={\"doc_id\":idx}\n",
    "    numOfWords[idx].update(tmp)\n",
    "    for word in lis:\n",
    "        numOfWords[idx][word] += 1\n",
    "#print(numOfWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTF(wordDict, bagOfWords):\n",
    "    tfDict = {}\n",
    "    bagOfWordsCount = len(bagOfWords)\n",
    "    for word, count in wordDict.items():\n",
    "        if word == 'doc_id':\n",
    "            tfDict[word] = count\n",
    "        else:\n",
    "            tfDict[word] = count / float(bagOfWordsCount)\n",
    "    return tfDict\n",
    "\n",
    "def computeIDF(dictn):\n",
    "    N = len(dictn)\n",
    "    idfDict = dict.fromkeys(dictn.keys(), 0)\n",
    "    \n",
    "    for word, val in dictn.items():\n",
    "        if word == 'doc_id':\n",
    "            idfDict[word] = val\n",
    "        elif val > 0:\n",
    "            idfDict[word] = idfDict[word] + 1\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        if word == 'doc_id':\n",
    "            idfDict[word] = val\n",
    "        else:\n",
    "            idfDict[word] = math.log(N / float(val))\n",
    "    return idfDict\n",
    "\n",
    "def computeTFIDF(tfBagOfWords, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBagOfWords.items():\n",
    "        if word == 'doc_id':\n",
    "            tfidf[word] = val\n",
    "        else:\n",
    "            tfidf[word] = val * idfs[word]\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = [computeTF(numOfWords[idx],list_new_lematized[idx]) for idx in range(len(numOfWords))]\n",
    "idfs = [computeIDF(numOfWords[idx]) for idx in range(len(numOfWords))] \n",
    "tfidf = [computeTFIDF(tf[idx], idfs[idx]) for idx in range(len(idfs))]\n",
    "\n",
    "df = pd.DataFrame(tfidf)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[]\n",
    "tfidf_list=[]\n",
    "c=0\n",
    "for doc in lematize_list:\n",
    "    x=[]\n",
    "    for sen in doc:\n",
    "        dic_temp={}\n",
    "        for word in sen:\n",
    "            for k,v in tfidf[idx].items():\n",
    "                if word == k:\n",
    "                    dic_temp[word]=v   \n",
    "            dic_temp['doc_id'] = i\n",
    "        x.append(dic_temp)\n",
    "        \n",
    "    tfidf_list.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sentence(doc,i,idx):\n",
    "    temp_dict={}\n",
    "    temp = 0\n",
    "    for key,val in doc.items():\n",
    "        if key == 'doc_id':\n",
    "            pass\n",
    "        else:\n",
    "            temp += val\n",
    "        temp_dict={'doc_id':doc['doc_id'] , 'tfid_score':temp, 'key':  \"\".join(list_new_1[i][idx])}\n",
    "    return temp_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_score = []\n",
    "\n",
    "for i,doc in enumerate(tfidf_list):\n",
    "    tfidf_score_temp=[]\n",
    "    for idx,dic in enumerate(doc):\n",
    "        temp_score = make_sentence(dic,i,idx)\n",
    "        tfidf_score_temp.append(temp_score)\n",
    "    tfidf_score.append(tfidf_score_temp)\n",
    "#tfidf_document = [make_sentence(doc) for doc in tfidf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tfidf scoring of sentence in 4th paper text in list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tfidf_score[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary(doc_list):\n",
    "    c=0\n",
    "    sum = 0\n",
    "    summary = []\n",
    "    array =  []\n",
    "    for temp_dict in doc_list:\n",
    "        if temp_dict:\n",
    "            sum = sum + temp_dict['tfid_score']\n",
    "\n",
    "    avg = sum/len(doc_list)\n",
    "\n",
    "    for temp_dict in doc_list:\n",
    "        if temp_dict:\n",
    "            array.append(temp_dict['tfid_score'])\n",
    "        \n",
    "    stdev = statistics.stdev(array)\n",
    "    #avg+stdev+avg\n",
    "    thres = avg+stdev+avg\n",
    "    for temp_dict in doc_list:\n",
    "        if temp_dict:\n",
    "            if temp_dict['tfid_score'] >= thres:\n",
    "                summary.append(temp_dict['key'])\n",
    "                c=c+1\n",
    "    summary = '.'.join(summary)\n",
    "    return summary,c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extractive Summary for related keyword papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summ = []\n",
    "for i,doc in enumerate(tfidf_score):\n",
    "    summary = (get_summary(doc))\n",
    "    summ.append(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extractive summary of 4th paper in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summ[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No of sentences of the 4th paper summary in the extracted list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sentences:\",len(summ[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No of sentences of the 4th paper in the extracted list (original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sentences:\",len(list_new_1[3]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
